/* eslint-disable func-style */
/* eslint-disable require-jsdoc */
/* eslint-disable indent */
/* eslint-disable no-negated-condition */
/*
 * Resources:
 *  - Text to speech extension written by Scratch Team 2019
 *  - Speech to text extension written by Sayamindu Dasgupta <sayamindu@media.mit.edu>, April 2014
 *  - Knn Classifier model written by Katya3141 https://katya3141.github.io/scratch-gui/teachable-classifier/ August 2019
 */

require("regenerator-runtime/runtime");
const Runtime = require('../../engine/runtime');
const nets = require('nets');
const MathUtil = require('../../util/math-util');

const ArgumentType = require('../../extension-support/argument-type');
const BlockType = require('../../extension-support/block-type');
const Clone = require('../../util/clone');
const Cast = require('../../util/cast');
const formatMessage = require('format-message');
const Timer = require('../../util/timer');
const tf = require('@tensorflow/tfjs');
const knnClassifier = require('@tensorflow-models/knn-classifier');
const use = require('@tensorflow-models/universal-sentence-encoder');
const Papa = require('papaparse');
const { combineLocations } = require("@tensorflow/tfjs-core/dist/ops/axis_util");
const EXTENSION_ID = 'textClassification';


/**
 * Icon svg to be displayed in the blocks category menu, encoded as a data URI.
 * @type {string}
 */
// eslint-disable-next-line max-len
const menuIconURI = 'data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   id="Layer_1_1_"
   enable-background="new 0 0 64 64"
   height="72.780205"
   viewBox="0 0 9.1991234 9.0975256"
   width="73.592987"
   version="1.1"
   inkscape:version="0.48.4 r9939"
   sodipodi:docname="text-classification-blocks-menu.svg">
  <metadata
     id="metadata21">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <defs
     id="defs19" />
  <sodipodi:namedview
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1"
     objecttolerance="10"
     gridtolerance="10"
     guidetolerance="10"
     inkscape:pageopacity="0"
     inkscape:pageshadow="2"
     inkscape:window-width="1007"
     inkscape:window-height="783"
     id="namedview17"
     showgrid="false"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0"
     inkscape:zoom="10.429825"
     inkscape:cx="44.912112"
     inkscape:cy="33.108471"
     inkscape:window-x="627"
     inkscape:window-y="178"
     inkscape:window-maximized="0"
     inkscape:current-layer="Layer_1_1_" />
  <path
     sodipodi:type="arc"
     style="fill:#aaffcc;fill-opacity:1;stroke:#00aa44"
     id="path2993"
     sodipodi:cx="-4.3624892"
     sodipodi:cy="28.581223"
     sodipodi:rx="32.934399"
     sodipodi:ry="32.934399"
     d="m 28.571909,28.581223 a 32.934399,32.934399 0 1 1 -65.868797,0 32.934399,32.934399 0 1 1 65.868797,0 z"
     transform="matrix(0.13755698,0,0,0.13610137,5.2013733,0.65924101)" />
  <path
     d="m 2.9791233,6.137428 c 0,0.139175 -0.03202,0.269729 -0.09114,0.385503 -0.140407,0.283277 -0.433537,0.476644 -0.771006,0.476644 -0.476644,0 -0.862147,-0.385503 -0.862147,-0.862147 0,-0.476644 0.385503,-0.862147 0.862147,-0.862147 0.173661,0 0.333774,0.0505 0.468023,0.137943 0.237706,0.153955 0.394124,0.419989 0.394124,0.724204 z"
     id="path3"
     inkscape:connector-curvature="0"
     style="fill:#000080;fill-opacity:1" />
  <path
     d="m 7.5002307,3.8572393 c 0.476644,0 0.862147,0.385503 0.862147,0.862147 0,0.476644 -0.385503,0.862147 -0.862147,0.862147 -0.231548,0 -0.440926,-0.08991 -0.594881,-0.24017 -0.16504,-0.155186 -0.267266,-0.376881 -0.267266,-0.621977 0,-0.03941 0.0025,-0.07759 0.0086,-0.115774 0.02463,-0.192136 0.113311,-0.363334 0.243864,-0.493887 0.156418,-0.156418 0.370723,-0.252486 0.609661,-0.252486 z"
     id="path7"
     inkscape:connector-curvature="0"
     style="fill:#000080" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 1.6842184,3.641441 C 1.2629901,3.3627243 1.2006396,2.7462127 1.5557106,2.3707757 1.9022591,2.0043501 2.4951785,2.0633456 2.7794918,2.4925419 3.2449268,3.1951577 2.3803563,4.1020583 1.6842184,3.641441 z"
     id="path2988"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 4.3049044,2.5776682 C 4.0127706,2.3282715 3.9342559,2.0011231 4.0810112,1.6447697 4.3064184,1.0974329 4.9784113,0.92659387 5.3886513,1.3123316 6.1978274,2.0731787 5.1487365,3.2980525 4.3049044,2.5776682 z"
     id="path2990"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 4.3784972,4.9757011 C 3.8278365,4.6409554 3.8928456,3.8504273 4.4938182,3.5733694 4.866088,3.4017469 5.2893822,3.5302824 5.5101175,3.8819738 5.7301862,4.2326033 5.7089766,4.5244492 5.4425609,4.8115608 5.1456488,5.1315377 4.7382343,5.1943846 4.3784972,4.9757011 z"
     id="path2992"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 4.3246268,7.9820751 C 4.0366848,7.7379754 3.9592965,7.4177752 4.1039462,7.0689904 4.326119,6.5332782 4.9884695,6.3660676 5.392823,6.7436128 6.1903885,7.4883005 5.1563507,8.6871597 4.3246268,7.9820751 z"
     id="path2994"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#f6db40;fill-opacity:1;stroke:none"
     d=""
     id="path3001"
     inkscape:connector-curvature="0" />
  <path
     d="m 2.5436163,5.250648 c -0.129322,-0.06244 -0.27367,-0.09853 -0.42664,-0.09853 -0.543275,0 -0.985311,0.442035 -0.985311,0.985311 0,0.543276 0.442036,0.985311 0.985311,0.985311 0.340918,0 0.641807,-0.174154 0.818793,-0.438094 l 0.93075,0.465929 c -0.01601,0.07033 -0.02525,0.143362 -0.02525,0.218492 0,0.543276 0.442035,0.985311 0.985311,0.985311 0.543275,0 0.98531,-0.442035 0.98531,-0.985311 0,-0.204205 -0.06244,-0.394001 -0.16935,-0.551527 l 1.307877,-1.367489 c 0.164054,0.121686 0.366289,0.194722 0.585767,0.194722 0.543276,0 0.985311,-0.442035 0.985311,-0.98531 0,-0.543276 -0.442035,-0.985311 -0.985311,-0.985311 -0.22736,0 -0.436246,0.07809 -0.603256,0.207901 L 5.6039903,2.553115 c 0.129815,-0.16701 0.2079,-0.375896 0.2079,-0.603257 0,-0.543275 -0.442035,-0.98531102 -0.98531,-0.98531102 -0.543276,0 -0.985311,0.44203602 -0.985311,0.98531102 0,0.07488 0.0091,0.147674 0.02512,0.217877 L 2.9929113,2.48599 C 2.8292333,2.168104 2.4984153,1.949857 2.1169763,1.949857 c -0.543275,0 -0.985311,0.442035 -0.985311,0.985311 0,0.543276 0.442036,0.985311 0.985311,0.985311 0.140161,0 0.273301,-0.02981 0.394125,-0.08289 l 0.453366,0.763 z m -0.42664,1.625763 c -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 z m 0.928532,-0.412476 c 0.03609,-0.102349 0.05678,-0.211965 0.05678,-0.326507 0,-0.302121 -0.136958,-0.572589 -0.351633,-0.753393 l 0.354219,-0.547094 1.067954,1.797083 c -0.08991,0.08006 -0.165163,0.175878 -0.220956,0.283647 z m 1.781073,-3.528767 c 0.22736,0 0.436246,-0.07809 0.603256,-0.207901 l 1.329061,1.329061 c -0.07969,0.102473 -0.139298,0.220833 -0.173661,0.349663 L 5.8115223,4.299577 c -1.23e-4,-0.0032 3.69e-4,-0.0064 3.69e-4,-0.0096 0,-0.543275 -0.442035,-0.985311 -0.98531,-0.985311 -0.340918,0 -0.64193,0.174154 -0.818793,0.438094 L 3.8608533,3.66935 4.4000643,2.836639 c 0.129199,0.06244 0.273547,0.09853 0.426517,0.09853 z m 0.123164,3.457086 V 5.266659 c 0.400282,-0.05025 0.725927,-0.341163 0.827907,-0.723218 l 0.773715,0.106414 c 0,0.0032 -4.92e-4,0.0064 -4.92e-4,0.0096 0,0.235243 0.08301,0.451149 0.221079,0.620746 L 5.4787333,6.632416 C 5.3337693,6.503956 5.1513643,6.417619 4.9497453,6.392247 z M 4.8265813,3.550987 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z m -0.928532,0.412476 c -0.03609,0.102349 -0.05678,0.211965 -0.05678,0.326507 0,0.501523 0.376881,0.915847 0.862147,0.976813 v 1.125594 c -0.116759,0.01466 -0.226991,0.04964 -0.32737,0.101487 L 3.2542703,4.606132 3.7261113,3.877495 z m 0.928532,4.144586 c -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 z m 2.709604,-4.18757 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z M 4.8265813,1.210874 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z m -0.875818,1.188162 c 0.06023,0.116882 0.142623,0.220217 0.242263,0.304214 L 3.6392813,3.5585 3.0456323,3.261675 c 0.03596,-0.102349 0.05665,-0.211965 0.05665,-0.326507 0,-0.07488 -0.0091,-0.147673 -0.02512,-0.217877 z m -2.57277,0.536132 c 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 z m 1.344334,0.775809 c 0.08252,-0.06454 0.15494,-0.141392 0.213442,-0.228592 l 0.568648,0.284262 -0.390553,0.603133 z"
     id="path15"
     inkscape:connector-curvature="0"
     style="fill:#000000" />
</svg>
';

/**
 * Icon svg to be displayed at the left edge of each extension block, encoded as a data URI.
 * @type {string}
 */
// eslint-disable-next-line max-len
const blockIconURI = 'data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<svg
   xmlns:dc="http://purl.org/dc/elements/1.1/"
   xmlns:cc="http://creativecommons.org/ns#"
   xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
   xmlns:svg="http://www.w3.org/2000/svg"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   id="Layer_1_1_"
   enable-background="new 0 0 64 64"
   height="59.118649"
   viewBox="0 0 7.3898301 7.3898311"
   width="59.118641"
   version="1.1"
   inkscape:version="0.48.4 r9939"
   sodipodi:docname="text-classification-blocks-small.svg">
  <metadata
     id="metadata21">
    <rdf:RDF>
      <cc:Work
         rdf:about="">
        <dc:format>image/svg+xml</dc:format>
        <dc:type
           rdf:resource="http://purl.org/dc/dcmitype/StillImage" />
        <dc:title></dc:title>
      </cc:Work>
    </rdf:RDF>
  </metadata>
  <defs
     id="defs19" />
  <sodipodi:namedview
     pagecolor="#ffffff"
     bordercolor="#666666"
     borderopacity="1"
     objecttolerance="10"
     gridtolerance="10"
     guidetolerance="10"
     inkscape:pageopacity="0"
     inkscape:pageshadow="2"
     inkscape:window-width="1007"
     inkscape:window-height="783"
     id="namedview17"
     showgrid="false"
     fit-margin-top="0"
     fit-margin-left="0"
     fit-margin-right="0"
     fit-margin-bottom="0"
     inkscape:zoom="7.375"
     inkscape:cx="40.244264"
     inkscape:cy="27.374394"
     inkscape:window-x="627"
     inkscape:window-y="178"
     inkscape:window-maximized="0"
     inkscape:current-layer="Layer_1_1_" />
  <path
     d="m 1.847458,5.172881 c 0,0.139175 -0.03202,0.269729 -0.09114,0.385503 -0.140407,0.283277 -0.433537,0.476644 -0.771006,0.476644 -0.476644,0 -0.862147,-0.385503 -0.862147,-0.862147 0,-0.476644 0.385503,-0.862147 0.862147,-0.862147 0.173661,0 0.333774,0.0505 0.468023,0.137943 0.237706,0.153955 0.394124,0.419989 0.394124,0.724204 z"
     id="path3"
     inkscape:connector-curvature="0"
     style="fill:#000080;fill-opacity:1" />
  <path
     d="m 6.3685654,2.8926923 c 0.476644,0 0.862147,0.385503 0.862147,0.862147 0,0.476644 -0.385503,0.862147 -0.862147,0.862147 -0.231548,0 -0.440926,-0.08991 -0.594881,-0.24017 -0.16504,-0.155186 -0.267266,-0.376881 -0.267266,-0.621977 0,-0.03941 0.0025,-0.07759 0.0086,-0.115774 0.02463,-0.192136 0.113311,-0.363334 0.243864,-0.493887 0.156418,-0.156418 0.370723,-0.252486 0.609661,-0.252486 z"
     id="path7"
     inkscape:connector-curvature="0"
     style="fill:#000080" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 0.5525531,2.676894 C 0.1313248,2.3981773 0.0689743,1.7816657 0.4240453,1.4062287 0.7705938,1.0398031 1.3635132,1.0987986 1.6478265,1.5279949 2.1132615,2.2306107 1.248691,3.1375113 0.5525531,2.676894 z"
     id="path2988"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 3.1732391,1.6131212 C 2.8811053,1.3637245 2.8025906,1.0365761 2.9493459,0.68022271 3.1747531,0.13288591 3.846746,-0.03795312 4.256986,0.34778461 5.0661621,1.1086317 4.0170712,2.3335055 3.1732391,1.6131212 z"
     id="path2990"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 3.2468319,4.0111541 C 2.6961712,3.6764084 2.7611803,2.8858803 3.3621529,2.6088224 3.7344227,2.4371999 4.1577169,2.5657354 4.3784522,2.9174268 4.5985209,3.2680563 4.5773113,3.5599022 4.3108956,3.8470138 4.0139835,4.1669907 3.606569,4.2298376 3.2468319,4.0111541 z"
     id="path2992"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#000080;fill-opacity:1;stroke:none"
     d="M 3.1929615,7.0175281 C 2.9050195,6.7734284 2.8276312,6.4532282 2.9722809,6.1044434 3.1944537,5.5687312 3.8568042,5.4015206 4.2611577,5.7790658 5.0587232,6.5237535 4.0246854,7.7226127 3.1929615,7.0175281 z"
     id="path2994"
     inkscape:connector-curvature="0" />
  <path
     style="fill:#f6db40;fill-opacity:1;stroke:none"
     d=""
     id="path3001"
     inkscape:connector-curvature="0" />
  <path
     d="M 1.411951,4.286101 C 1.282629,4.223661 1.138281,4.187571 0.985311,4.187571 0.442036,4.187571 0,4.629606 0,5.172882 c 0,0.543276 0.442036,0.985311 0.985311,0.985311 0.340918,0 0.641807,-0.174154 0.818793,-0.438094 l 0.93075,0.465929 c -0.01601,0.07033 -0.02525,0.143362 -0.02525,0.218492 0,0.543276 0.442035,0.985311 0.985311,0.985311 0.543275,0 0.98531,-0.442035 0.98531,-0.985311 0,-0.204205 -0.06244,-0.394001 -0.16935,-0.551527 L 5.818752,4.485504 c 0.164054,0.121686 0.366289,0.194722 0.585767,0.194722 0.543276,0 0.985311,-0.442035 0.985311,-0.98531 0,-0.543276 -0.442035,-0.985311 -0.985311,-0.985311 -0.22736,0 -0.436246,0.07809 -0.603256,0.207901 L 4.472325,1.588568 C 4.60214,1.421558 4.680225,1.212672 4.680225,0.98531101 4.680225,0.44203601 4.23819,0 3.694915,0 3.151639,0 2.709604,0.44203601 2.709604,0.98531101 c 0,0.07488 0.0091,0.14767399 0.02512,0.21787699 L 1.861246,1.521443 C 1.697568,1.203557 1.36675,0.98531001 0.985311,0.98531001 0.442036,0.98531001 0,1.427345 0,1.970621 c 0,0.543276 0.442036,0.985311 0.985311,0.985311 0.140161,0 0.273301,-0.02981 0.394125,-0.08289 l 0.453366,0.763 z m -0.42664,1.625763 c -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 z M 1.913843,5.499388 C 1.949933,5.397039 1.970623,5.287423 1.970623,5.172881 1.970623,4.87076 1.833665,4.600292 1.61899,4.419488 L 1.973209,3.872394 3.041163,5.669477 C 2.951253,5.749537 2.876,5.845355 2.820207,5.953124 z M 3.694916,1.970621 c 0.22736,0 0.436246,-0.07809 0.603256,-0.207901 L 5.627233,3.091781 C 5.547543,3.194254 5.487935,3.312614 5.453572,3.441444 L 4.679857,3.33503 c -1.23e-4,-0.0032 3.69e-4,-0.0064 3.69e-4,-0.0096 0,-0.543275 -0.442035,-0.985311 -0.98531,-0.985311 -0.340918,0 -0.64193,0.174154 -0.818793,0.438094 L 2.729188,2.704803 3.268399,1.872092 c 0.129199,0.06244 0.273547,0.09853 0.426517,0.09853 z M 3.81808,5.427707 V 4.302112 C 4.218362,4.251862 4.544007,3.960949 4.645987,3.578894 l 0.773715,0.106414 c 0,0.0032 -4.92e-4,0.0064 -4.92e-4,0.0096 0,0.235243 0.08301,0.451149 0.221079,0.620746 L 4.347068,5.667869 C 4.202104,5.539409 4.019699,5.453072 3.81808,5.4277 z M 3.694916,2.58644 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z M 2.766384,2.998916 c -0.03609,0.102349 -0.05678,0.211965 -0.05678,0.326507 0,0.501523 0.376881,0.915847 0.862147,0.976813 V 5.42783 C 3.454992,5.44249 3.34476,5.47747 3.244381,5.529317 L 2.122605,3.641585 2.594446,2.912948 z m 0.928532,4.144586 c -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 z M 6.40452,2.955932 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z M 3.694916,0.24632701 c 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.40754899 -0.331434,0.73898299 -0.738983,0.73898299 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.73898299 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 z M 2.819098,1.434489 c 0.06023,0.116882 0.142623,0.220217 0.242263,0.304214 L 2.507616,2.593953 1.913967,2.297128 c 0.03596,-0.102349 0.05665,-0.211965 0.05665,-0.326507 0,-0.07488 -0.0091,-0.147673 -0.02512,-0.217877 z m -2.57277,0.536132 c 0,-0.407549 0.331434,-0.738983 0.738983,-0.738983 0.407549,0 0.738983,0.331434 0.738983,0.738983 0,0.407549 -0.331434,0.738983 -0.738983,0.738983 -0.407549,0 -0.738983,-0.331434 -0.738983,-0.738983 z M 1.590662,2.74643 C 1.673182,2.68189 1.745602,2.605038 1.804104,2.517838 L 2.372752,2.8021 1.982199,3.405233 z"
     id="path15"
     inkscape:connector-curvature="0"
     style="fill:#000000" />
</svg>
';

/**
 * The url of the synthesis server.
 * @type {string}
 */
const SERVER_HOST = 'https://synthesis-service.scratch.mit.edu';

/**
 * The url of the translate server.
 * @type {string}
 */
const serverURL = 'https://translate-service.scratch.mit.edu/';

/**
 * How long to wait in ms before timing out requests to translate server.
 * @type {int}
 */
const serverTimeoutMs = 10000; // 10 seconds (chosen arbitrarily).


/**
 * How long to wait in ms before timing out requests to synthesis server.
 * @type {int}
 */
const SERVER_TIMEOUT = 10000; // 10 seconds

/**
 * Volume for playback of speech sounds, as a percentage.
 * @type {number}
 */
const SPEECH_VOLUME = 250;

/**
 * An id for one of the voices.
 */
const ALTO_ID = 'ALTO';

/**
 * An id for one of the voices.
 */
const TENOR_ID = 'TENOR';

/**
 * An id for one of the voices.
 */
const SQUEAK_ID = 'SQUEAK';

/**
 * An id for one of the voices.
 */
const GIANT_ID = 'GIANT';

/**
 * Playback rate for the tenor voice, for cases where we have only a female gender voice.
 */
const FEMALE_TENOR_RATE = 0.89; // -2 semitones

/**
 * Playback rate for the giant voice, for cases where we have only a female gender voice.
 */
const FEMALE_GIANT_RATE = 0.79; // -4 semitones


/**
 * Class for the motion-related blocks in Scratch 3.0
 * @param {Runtime} runtime - the runtime instantiating this block package.
 * @constructor
 */
class Scratch3TextClassificationBlocks {
    constructor (runtime) {

        /**
         * The result from the most recent translation.
         * @type {string}
         * @private
         */
        this._translateResult = '';

        /**
         * The language of the text most recently translated.
         * @type {string}
         * @private
         */
        this._lastLangTranslated = '';

        /**
         * The text most recently translated.
         * @type {string}
         * @private
         */
        this._lastTextTranslated = '';

        /**
         * The runtime instantiating this block package.
         * @type {Runtime}
         */
        this.scratch_vm = runtime;
        this.predictedLabel = null;
        this.classifier = knnClassifier.create();
        this.embedding = [];
        this.count = 0;
        this.confidence = 0;
        this.classifiedData = null;
        this._mStatus = 1;
        this.scratch_vm.registerPeripheralExtension(EXTENSION_ID, this);
        this.scratch_vm.connectPeripheral(EXTENSION_ID, 0);
        this.similarity = 0;
        this.k = 0;
        this.labelList = [''];
        this.labelListEmpty = true;
        this.exampleEmbeddings = {};
        this.lastEmbedding = {};
        
         
        /**
         * The timer utility.
         * @type {Timer}
         */
        this._timer = new Timer();

        /**
         * The stored microphone loudness measurement.
         * @type {number}
         */
        this._cachedLoudness = -1;

        /**
         * The time of the most recent microphone loudness measurement.
         * @type {number}
         */
        this._cachedLoudnessTimestamp = 0;
         
        /**
         * Map of soundPlayers by sound id.
         * @type {Map<string, SoundPlayer>}
         */
        this._soundPlayers = new Map();

        this._stopAllSpeech = this._stopAllSpeech.bind(this);
        if (this.scratch_vm) {
            this.scratch_vm.on('PROJECT_STOP_ALL', this._stopAllSpeech);
        }

        this._onTargetCreated = this._onTargetCreated.bind(this);
        if (this.scratch_vm) {
            this.scratch_vm.on('targetWasCreated', this._onTargetCreated);
        }
        
        this.scratch_vm.on('EDIT_TEXT_MODEL', modelInfo => {
            this.editModel.bind(this, modelInfo);
        });
        this.scratch_vm.on('EDIT_TEXT_CLASSIFIER', modelInfo => {
            this.editModel.bind(this, modelInfo);
        });
        
        // When a project is loaded, reset all the model data
        this.scratch_vm.on('PROJECT_LOADED', () => {
            this.clearLocal();
            this.loadModelFromRuntime();
        });

        // Listen for model editing events emitted by the text modal
        this.scratch_vm.on('NEW_EXAMPLES', (examples, label) => {
            this.newExamples(examples, label);
        });
        this.scratch_vm.on('NEW_LABEL', label => {
            this.newLabel(label);
        });
        this.scratch_vm.on('DELETE_EXAMPLE', (label, exampleNum) => {
            this.deleteExample(label, exampleNum);
        });
        this.scratch_vm.on('RENAME_LABEL', (oldName, newName) => {
            this.renameLabel(oldName, newName);
        });
        this.scratch_vm.on('DELETE_LABEL', label => {
            this.clearAllWithLabel({LABEL: label});
        });
        // this.scratch_vm.on('CLEAR_ALL_LABELS', () => {
        //     // confirm with alert dialogue before clearing the model
        //     if (!this.labelListEmpty && confirm('Are you sure you want to clear all labels?')) {
        //         this.clearAll();
        //     }
        // });

        // Listen for model editing events emitted by the classifier modal
        this.scratch_vm.on('EXPORT_CLASSIFIER', () => {
            this.exportClassifier();
        });
        this.scratch_vm.on('LOAD_CLASSIFIER', () => {
            console.log('load');
            this.loadClassifier();
        });

        
        this._recognizedSpeech = '';
    }

    /**
     * An object with info for each voice.
     */
    get VOICE_INFO () {
        return {
            [SQUEAK_ID]: {
                name: formatMessage({
                    id: 'text2speech.squeak',
                    default: 'squeak',
                    description: 'Name for a funny voice with a high pitch.'
                }),
                gender: 'female',
                playbackRate: 1.19 // +3 semitones
            },
            [TENOR_ID]: {
                name: formatMessage({
                    id: 'text2speech.tenor',
                    default: 'tenor',
                    description: 'Name for a voice with ambiguous gender.'
                }),
                gender: 'male',
                playbackRate: 1
            },
            [ALTO_ID]: {
                name: formatMessage({
                    id: 'text2speech.alto',
                    default: 'alto',
                    description: 'Name for a voice with ambiguous gender.'
                }),
                gender: 'female',
                playbackRate: 1
            },
            [GIANT_ID]: {
                name: formatMessage({
                    id: 'text2speech.giant',
                    default: 'giant',
                    description: 'Name for a funny voice with a low pitch.'
                }),
                gender: 'male',
                playbackRate: 0.84 // -3 semitones
            }
        };
    }
    
    /**
    * The key to load & store a target's text2speech state.
    * @return {string} The key.
    */
    static get STATE_KEY () {
        return 'Scratch.text2speech';
    }

    /**
    * The default state, to be used when a target has no existing state.
    * @type {Text2SpeechState}
    */
    static get DEFAULT_TEXT2SPEECH_STATE () {
        return {
            voiceId: SQUEAK_ID
        };
    }

    async loadUSEModel () {
        const retries = 10;
        let exponentialBackoffDelay = 1000;

        for (let i = 0; i < retries; i++) {
            try {
                const model = await use.load();
                return model;
            }
            catch (error) {
                console.error('Error loading USE model:', error);
                console.warn('Retrying in ' + exponentialBackoffDelay / 1000 + 's');
                if (i < retries - 1) {
                    await new Promise(resolve => setTimeout(resolve, exponentialBackoffDelay));
                    exponentialBackoffDelay *= 2;
                }
            }
        }
    }

    _useModel = null;

    get useModelPromise () {
        if (!this._useModel) this._useModel = this.loadUSEModel();
        return this._useModel;
    }
    
    /**
     * @param {Target} target - collect  state for this target.
     * @returns {Text2SpeechState} the mutable state associated with that target. This will be created if necessary.
     * @private
     */
    _getState (target) {
        let state = target.getCustomState(Scratch3TextClassificationBlocks.STATE_KEY);
        if (!state) {
            state = Clone.simple(Scratch3TextClassificationBlocks.DEFAULT_TEXT2SPEECH_STATE);
            target.setCustomState(Scratch3TextClassificationBlocks.STATE_KEY, state);
        }
        return state;
    }

    /**
     * When a Target is cloned, clone the state.
     * @param {Target} newTarget - the newly created target.
     * @param {Target} [sourceTarget] - the target used as a source for the new clone, if any.
     * @listens Runtime#event:targetWasCreated
     * @private
     */
    _onTargetCreated (newTarget, sourceTarget) {
        if (sourceTarget) {
            const state = sourceTarget.getCustomState(Scratch3TextClassificationBlocks.STATE_KEY);
            if (state) {
                newTarget.setCustomState(Scratch3TextClassificationBlocks.STATE_KEY, Clone.simple(state));
            }
        }
    }
    
    /**
     * @returns {object} metadata for this extension and its blocks.
     */
    getInfo () {
        // Set the video display properties to defaults the first time
        // getInfo is run. This turns on the video device when it is
        // first added to a project, and is overwritten by a PROJECT_LOADED
        // event listener that later calls updateVideoDisplay
        if (this.firstInstall) {
            this.globalVideoState = VideoState.ON;
            this.globalVideoTransparency = 50;
            this.updateVideoDisplay();
            this.firstInstall = false;
            this.predictionState = {};
        }

        // Return extension definition
        return {
            id: 'textClassification',
            name: formatMessage({
                id: 'textClassification.categoryName',
                default: 'Text Classification',
                description: 'Label for the Text Classification extension category'
            }),
            blockIconURI: blockIconURI,
            showStatusButton: true,
            menuIconURI: menuIconURI,
            blocks: [
                {
                    func: 'EDIT_TEXT_MODEL',
                    blockType: BlockType.BUTTON,
                    text: 'Edit Model'
                },
                {
                    func: 'EDIT_TEXT_CLASSIFIER',
                    blockType: BlockType.BUTTON,
                    text: 'Load / Save Model'
                },
                
                {
                    opcode: 'ifTextMatchesClass',
                    text: formatMessage({
                        id: 'textClassification.ifTextMatchesClass',
                        default: '[TEXT] matches [CLASS_NAME] ?',
                        description: 'Conditional that is true when the text matches the text classification model class [CLASS_NAME]'
                    }),
                    blockType: BlockType.BOOLEAN,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        },
                        CLASS_NAME: {
                            type: ArgumentType.STRING,
                            menu: 'model_classes',
                            defaultValue: this.getLabels()[0]
                        }
                    }
                },
                {
                    opcode: 'getModelPrediction',
                    text: formatMessage({
                        id: 'textClassification.getModelPrediction',
                        default: 'predict class for [TEXT]',
                        description: 'Get the class name that the input text matches'
                    }),
                    blockType: BlockType.REPORTER,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        }
                    }
                },
                {
                    opcode: 'getConfidence',
                    text: formatMessage({
                        id: 'textClassification.getConfidence',
                        default: 'get confidence for [TEXT]',
                        description: 'get the confidence for the labeling of a specified text'
                    }),
                    blockType: BlockType.REPORTER,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        }
                    }
                },
                '---',
                {
                    opcode: 'getSimilarity',
                    text: formatMessage({
                        id: 'textClassification.getSimilarity',
                        default: 'similarity for [TEXT] and [TEXT_TWO]',
                        description: 'get the similarity between two words/sentences'
                    }),
                    blockType: BlockType.REPORTER,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        },
                        TEXT_TWO: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        }
                    }
                },
                {
                    opcode: 'getNearestNeighbors',
                    text: formatMessage({
                        id: 'textClassification.getNearestNeighbors',
                        default: 'get nearest neighbors for [TEXT]',
                        description: 'get the nearest neighbors for inputted word'
                    }),
                    blockType: BlockType.REPORTER,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'text'
                        }
                    }
                },
                /*'---',
                {
                    opcode: 'speakText',
                    text: formatMessage({
                        id: 'textClassification.speakText',
                        default: 'speak [TEXT]',
                        description: 'Send text to the speech to text engine'
                    }),
                    blockType: BlockType.COMMAND,
                    arguments: {
                        TEXT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'Hello'
                        }
                    }
                },
                {
                    opcode: 'askSpeechRecognition',
                    text: formatMessage({
                        id: 'textClassification.askSpeechRecognition',
                        default: 'ask [PROMPT] and wait',
                        description: 'Get the class name that the input text matches'
                    }),
                    blockType: BlockType.COMMAND,
                    arguments: {
                        PROMPT: {
                            type: ArgumentType.STRING,
                            defaultValue: 'How are you?'
                        }
                    }
                },
                {
                    opcode: 'getRecognizedSpeech',
                    text: formatMessage({
                        id: 'textClassification.getRecognizedSpeech',
                        default: 'answer',
                        description: 'Return the results of the speech recognition'
                    }),
                    blockType: BlockType.REPORTER
                },
                {
                    opcode: 'setVoice',
                    text: formatMessage({
                        id: 'text2speech.setVoiceBlock',
                        default: 'set voice to [VOICE]',
                        description: 'Set the voice for speech synthesis.'
                    }),
                    blockType: BlockType.COMMAND,
                    arguments: {
                        VOICE: {
                            type: ArgumentType.STRING,
                            menu: 'voices',
                            defaultValue: SQUEAK_ID
                        }
                    }
                },
                '---',
                {
                    opcode: 'onHeardSound',
                    text: formatMessage({
                        id: 'textClassification.onHeardSound',
                        default: 'when heard sound > [THRESHOLD]',
                        description: 'Event that triggers when a sound is heard above a threshold'
                    }),
                    blockType: BlockType.HAT,
                    arguments: {
                        THRESHOLD: {
                            type: ArgumentType.NUMBER,
                            defaultValue: 10
                        }
                    }
                }*/
            ],
            menus: {
                voices: {
                    acceptReporters: true,
                    items: this.getVoiceMenu()
                },
                model_classes: {
                    acceptReporters: false,
                    items: 'getLabels'
                }
            }
        };
    }
    connect () {
    }
    disconnect () {
    }
    scan () {
        
    }
    isConnected () {
        return (this._mStatus === 2);
    }
    
    onDeviceDisconnected () {
        console.log('Lost connection to robot');
        this.scratch_vm.emit(this.scratch_vm.constructor.PERIPHERAL_DISCONNECTED);
        this._mDevice = null;
        this._mServices = null;
        this._mStatus = 1;
    }
    
    /**
     * Moves info from the runtime into the classifier, called when a project is loaded
    */ 
    loadModelFromRuntime () {
        this.labelList = [];
        this.labelListEmpty = false;
        const textData = this.scratch_vm.modelData.textData;

        for (const label in this.scratch_vm.modelData.textData) {
            if (this.scratch_vm.modelData.textData.hasOwnProperty(label)) {
                const textExamples = textData[label];
                this.newLabel(label);
                this.newExamples(textExamples, label);
            }
        }

        if (this.labelList.length === 0) {
            // if the label list is empty, fill it with an empty string
            this.labelList.push('');
            this.labelListEmpty = true;
        }
    }

    /**
     * Return label list for block menus
     * @return {array of strings} an array of the labels for the text model classifier
     */
    getLabels () {
        return this.labelList;
    }

    /**
     * Grab text and add it as an example
     * @param {string} args.LABEL the name of the label to add an example to
     */
    textExample (args) {
        const text = '';
         if (frame) {
             this.newExamples([text], args.LABEL);
         }
    }

    /**
     * Add new examples to a label
     * @param {array of strings} examples a list of text examples to add to a label
     * @param {string} label the name of the label
     */
    newExamples (textExamples, label) {   //add examples for a label
        if (this.labelListEmpty) {
            // Edit label list accordingly
            this.labelList.splice(this.labelList.indexOf(''), 1);
            this.labelListEmpty = false;
        }
        if (!this.labelList.includes(label)) {
            this.labelList.push(label);
        }
        for (let textExample of textExamples) {
            if (!this.scratch_vm.modelData.textData[label].includes(textExample)) {
                const embeddedExample = this.getembeddedwords(textExample, label, 'example'); // delayed by one example
                this.scratch_vm.modelData.textData[label].push(textExample);
                this.scratch_vm.modelData.classifierData[label].push(textExample);
                this.count++;
            }
        }

    }
    
    /**
     * Add a new label to labelList
     * @param {string} label the name of the label
     */
    newLabel (newLabelName) {   // add the name of a new label
        if (this.labelListEmpty) {
            // Edit label list accordingly
            this.labelList.splice(this.labelList.indexOf(''), 1);
            this.labelListEmpty = false;
        }
        if (!this.labelList.includes(newLabelName)) {
            this.labelList.push(newLabelName);
        }
        
        this.scratch_vm.modelData.textData[newLabelName] = [];
        this.scratch_vm.modelData.classifierData[newLabelName] = [];
        this.scratch_vm.requestToolboxExtensionsUpdate();
    }
    

    /**
     * @param {string} oldName the name of the label to change
     * @param {string} newname the new name for the label
     */
    renameLabel (oldName, newName) {
        const data = {...this.classifier.getClassifierDataset()};  // reset the classifier dataset with the renamed label
        if (data[oldName]) {
            data[newName] = data[oldName];
            delete data[oldName];
            this.classifier.setClassifierDataset(data);
        }

        // reset the runtime's model data with the new renamed label (to share with GUI)
        this.scratch_vm.modelData.classifierData[newName] = this.scratch_vm.modelData.classifierData[oldName];
        delete this.scratch_vm.modelData.classifierData[oldName];

        // reset the runtime's model data with the new renamed label (to share with GUI)
        this.scratch_vm.modelData.textData[newName] = this.scratch_vm.modelData.textData[oldName];
        delete this.scratch_vm.modelData.textData[oldName];

        // reset label list with the new renamed label
        this.labelList.splice(this.labelList.indexOf(oldName), 1);
        this.labelList.push(newName);
    }

    /**
     * Delete an example (or all loaded examples, if exampleNum === -1)
     * @param {string} label the name of the label with the example to be removed
     * @param {integer} exampleNum which example, in the array of a label's examples, to remove
     */
    deleteExample (label, exampleNum) {
        // reset the classifier dataset with the deleted example
        const data = {...this.classifier.getClassifierDataset()};
        const labelExamples = data[label].arraySync();
        // Remove label from the runtime's model data (to share with the GUI)
        // if this is true, delete all the loaded examples
        if (exampleNum === -1) {
            // imageData[label].length is ONLY the length of the NEW examples (not the saved and then loaded ones!)
            const numLoadedExamples = this.scratch_vm.modelData.classifierData[label].length - this.scratch_vm.modelData.textData[label].length;
            this.scratch_vm.modelData.classifierData[label].splice(0, numLoadedExamples);
            labelExamples.splice(0, numLoadedExamples);
        } else {
            this.scratch_vm.modelData.textData[label].splice(exampleNum, 1);
            this.scratch_vm.modelData.classifierData[label].splice(exampleNum - this.scratch_vm.modelData.textData[label].length - 1, 1);
            labelExamples.splice(exampleNum - this.scratch_vm.modelData.textData[label].length - 1, 1);
        }

        if (labelExamples.length > 0) {
            data[label] = tf.tensor(labelExamples);
            this.classifier.setClassifierDataset(data);
        } else {
            // if there are no more examples for this label, don't consider it in the classifier anymore (but keep it in labelList and the runtime model data)
            this.classifier.clearClass(label);
        }

    }

    /**
     * Clear all data stored in the classifier and label list
     */
    clearLocal () {
        this.scratch_vm.emit("TOOLBOX_EXTENSIONS_NEED_UPDATE");
        for (const label of this.labelList) {
            this.clearAllWithLabel({LABEL: label});
            console.log("clear local:", label);
        }
        this.labelList = [''];
        this.labelListEmpty = true;

        // clear saved embeddings
        this.exampleEmbeddings = {};
    }

    /**
     * Clear local label list, but also clear all data stored in the runtime
     */
    clearAll () {
        for (const label of this.labelList) {
            this.clearAllWithLabel({LABEL: label});
        }
        
    }

    /**
     * TODO Clear all examples with a given label
     * @param {string} args.LABEL the name of the label to remove from the model
     */
    clearAllWithLabel (args) {
        if (this.labelList.includes(args.LABEL)) {
            if (this.classifier.getClassExampleCount()[args.LABEL] > 0) {
                // remove label from the classifier
                this.classifier.clearClass(args.LABEL);
            }
            // Remove label from labelList
            this.labelList.splice(this.labelList.indexOf(args.LABEL), 1);
            // Remove label from the runtime's model data (to share with the GUI)
            delete this.scratch_vm.modelData.classifierData[args.LABEL];
            delete this.scratch_vm.modelData.textData[args.LABEL];
            // If the label list is now empty, fill it with an empty string
            if (this.labelList.length === 0) {
                this.labelListEmpty = true;
                this.labelList.push('');
            }
        }
    }
    
    
    /**
     * Detects if the sound from the input mic is louder than a particular threshold
     * @param args.THRESHOLD {integer} the threshold of loudness to trigger on
     * @return {integer} true if the loudness is above the threshold and false if it is not
     */    
    onHeardSound(args) {
        let threshold = args.THRESHOLD;
        
        return this.getLoudness() > threshold;
    }
    
    /**
     * Get the input volume from the mic
     * @return {integer} mic volume at current time
     */
    getLoudness () {
        if (typeof this.scratch_vm.audioEngine === 'undefined') return -1;
        if (this.scratch_vm.currentStepTime === null) return -1;

        // Only measure loudness once per step
        const timeSinceLoudness = this._timer.time() - this._cachedLoudnessTimestamp;
        if (timeSinceLoudness < this.scratch_vm.currentStepTime) {
            return this._cachedLoudness;
        }

        this._cachedLoudnessTimestamp = this._timer.time();
        this._cachedLoudness = this.scratch_vm.audioEngine.getLoudness();
        return this._cachedLoudness;
    }
    
    /**
     * Get the menu of voices for the "set voice" block.
     * @return {array} the text and value for each menu item.
     */
    getVoiceMenu () {
        return Object.keys(this.VOICE_INFO).map(voiceId => ({
            text: this.VOICE_INFO[voiceId].name,
            value: voiceId
        }));
    }
    
    /**
     * Set the voice for speech synthesis for this sprite.
     * @param  {object} args Block arguments
     * @param {object} util Utility object provided by the runtime.
     */
    setVoice (args, util) {
        const state = this._getState(util.target);

        let voice = args.VOICE;

        // If the arg is a dropped number, treat it as a voice index
        let voiceNum = parseInt(voice, 10);
        if (!isNaN(voiceNum)) {
            voiceNum -= 1; // Treat dropped args as one-indexed
            voiceNum = MathUtil.wrapClamp(voiceNum, 0, Object.keys(this.VOICE_INFO).length - 1);
            voice = Object.keys(this.VOICE_INFO)[voiceNum];
        }

        // Only set the voice if the arg is a valid voice id.
        if (Object.keys(this.VOICE_INFO).includes(voice)) {
            state.voiceId = voice;
        }
    }
    
    /**
     * Stop all currently playing speech sounds.
     */
    _stopAllSpeech () {
        this._soundPlayers.forEach(player => {
            player.stop();
        });
    }

    /**
     * Convert the provided text into a sound file and then play the file.
     * @param  {object} args Block arguments
     * @param {object} util Utility object provided by the runtime.
     * @return {Promise} A promise that resolves after playing the sound
     */
    async speakText(args, util) {
        // Cast input to string
        const words = Cast.toString(args.TEXT);
        const locale = 'en-US';

        const state = this._getState(util.target);

        const gender = this.VOICE_INFO[state.voiceId].gender;
        const playbackRate = this.VOICE_INFO[state.voiceId].playbackRate;
        
        // Build up URL
        let path = `${SERVER_HOST}/synth`;
        path += `?locale=${locale}`;
        path += `&gender=${gender}`;
        path += `&text=${encodeURIComponent(words.substring(0, 128))}`;
        // Perform HTTP request to get audio file
        return new Promise(resolve => {
            nets({
                url: path,
                timeout: SERVER_TIMEOUT
            }, (err, res, body) => {
                if (err) {
                    console.warn(err);
                    return resolve();
                }

                if (res.statusCode !== 200) {
                    console.warn(res.statusCode);
                    return resolve();
                }

                // Play the sound
                const sound = {
                    data: {
                        buffer: body.buffer
                    }
                };
                this.scratch_vm.audioEngine.decodeSoundPlayer(sound).then(soundPlayer => {
                    this._soundPlayers.set(soundPlayer.id, soundPlayer);

                    soundPlayer.setPlaybackRate(playbackRate);

                    // Increase the volume
                    const engine = this.scratch_vm.audioEngine;
                    const chain = engine.createEffectChain();
                    chain.set('volume', SPEECH_VOLUME);
                    soundPlayer.connect(chain);

                    soundPlayer.play();
                    soundPlayer.on('stop', () => {
                        this._soundPlayers.delete(soundPlayer.id);
                        resolve();
                    });
                });
            });
        });
    }
    
    recognizeSpeech () {
        const recognition = new webkitSpeechRecognition();
        const self = this;
        
        return new Promise(resolve => {
            recognition.start();
            recognition.onresult = function (event) {
                if (event.results.length > 0) {
                    self._recognizedSpeech = event.results[0][0].transcript;
                }
                resolve();
            };
        });
    }
    
    async askSpeechRecognition (args, util) {
        const prompt = Cast.toString(args.PROMPT);
        args.TEXT = prompt;
        const speakTextResolved = await this.speakText(args, util);
        return this.recognizeSpeech();
     }
    
    getRecognizedSpeech () {
        return this._recognizedSpeech;
    }

    /**
     * A scratch conditional block that checks if a text example is a part of a particular class
     * @param {object} args - the block arguments
     * @param {BlockUtility} util - the block utility
     * @returns {boolean} true if the model matches
     *   reference
     */
    async ifTextMatchesClass (args) {
        const text = args.TEXT;
        const className = args.CLASS_NAME;

        if (className) {
            let output = await this.get_embeddings(text, 'none', 'predict');
            let label = await Promise.all([this.predictedLabel]);
            return String(label[0]) === String(className);
        }

        return false;
    }

    timeout (delay) {
        return new Promise(res => setTimeout(res, delay));
    }

    /**
     * A scratch hat block reporter that returns whether the input text matches the model class.
     * @param {object} args - the block arguments
     * @param {BlockUtility} util - the block utility
     * @returns {string} class name if input text matched, empty string if there's a problem with the model
     */
    async getModelPrediction (args) {
        const text = args.TEXT;
        console.log("getModelPrediction waiting for get_embeddings");

        await this.get_embeddings(text, 'none', 'predict');
        console.log("getModelPediction after:", this.predictedLabel);

        return this.predictedLabel;
    }

    async getConfidence (args) {
        const text = args.TEXT;
        console.log("getConfidence waiting for get_embeddings:", this.confidence);

        await this.get_embeddings(text, 'none', 'predict');
        console.log("getConfidence after:", this.confidence);
        
        return this.confidence;
    }

    async getNearestNeighbors (args) {
       // get k parameter
        const text = args.TEXT;
        let count = 0;
        for (const label in this.scratch_vm.modelData.textData) {
            for (const _ of this.scratch_vm.modelData.textData[label]) {
                count = count + 1;
            }
        }
        this.k = Math.floor(Math.sqrt(count));
        
        // get similarity for word to other words, keep k highest
        const allSimilarities = {};

        // create a list to store all of the promises
        const promises = [];
        // also create a list to store the words associated with that promise
        const words = [];

        // get embeddings for comparison text once at the beginning and save it (more optimized)
        let translatedText = await this.getTranslate(text, 'en');
        let useModel = await this.useModelPromise;
        this.lastEmbedding[translatedText] = await useModel.embed(translatedText);
        
        for (const label in this.scratch_vm.modelData.textData) {
            for (const word of this.scratch_vm.modelData.textData[label]) {
                let similarity = this.getSimilarityOutput(word, text);
                promises.push(similarity);

                // also push the words into an array to keep them in order
                words.push(word);
            }
        }

        // wait for all of the promises to resolve, then reassociate promises with words
        let results = await Promise.all(promises)
        for (let i = 0; i < results.length; i++){
            const word = words[i];
            const similarity = results[i];
            allSimilarities[word] = similarity;
        }

        // go through similarities and find nearest k words
        let nearest = this.k;
        const nearestWords = [];
        while (nearest > 0 && Object.keys(allSimilarities).length) {
            const maxNearest = Object.keys(allSimilarities).reduce((a, b) => allSimilarities[a] > allSimilarities[b] ? a : b);
            nearest = nearest - 1;
            nearestWords.push(` ${maxNearest}`);
            delete allSimilarities[maxNearest];
        }

        if (nearestWords.length === 0) {
            return 'No neighbors found';
        }
        return nearestWords;

    }

    async getSimilarity (args) {
        const firstText = args.TEXT;
        const secondText = args.TEXT_TWO;
        await this.getSimilarityOutput(firstText, secondText);
        return this.similarity.toFixed(2);
    }

    async getSimilarityOutput (firstText, secondText) {        
        // translates text from any language to english
        let newFirstText = await this.getTranslate(firstText, 'en');
        let newSecondText = await this.getTranslate(secondText, 'en');

        let firstEmbedding = this.exampleEmbeddings[newFirstText] || this.lastEmbedding[newFirstText];
        let secondEmbedding = this.exampleEmbeddings[newSecondText] || this.lastEmbedding[newSecondText];

        if (!firstEmbedding || !secondEmbedding) {
            let useModel = await this.useModelPromise;
            if (!firstEmbedding) firstEmbedding = await useModel.embed(newFirstText);
            if (!secondEmbedding) secondEmbedding = await useModel.embed(newSecondText);    
        }
            
        const distance = tf.losses.cosineDistance(firstEmbedding, secondEmbedding, 1).dataSync();
        this.similarity = 1 - distance[0];
        return this.similarity.toFixed(2);
    }
    

    /**
     * Returns whether or not the text inputted is one of the examples inputted
     * @param text - the text inputted
     * @param className - the class whose examples are being checked
     * @returns a boolean true if the text is an example or false if the text is not an example
     */
    getPredictedClass (text, className) {
        // whenever the classifier has some data
        if (!this.labelListEmpty) {
            try {
                for (let example of this.scratch_vm.modelData.textData[className]) {
                    if (text.toLowerCase() === example.toLowerCase()) {
                        return true;
                    }
                }
            } catch(err) {
                return false;
            }
        } else { //if there is no data in the classifier
            return false;
        }
    

    }

    /**
     * Calls the method which embeds text as a 2d tensor
     * @param text - the text inputted
     * @param label - this is always "none" when embedding examples
     * @param direction - is either "example" when an example is being inputted or "predict" when a word to be classified is inputted
     */
    async getembeddedwords (text, label, direction) {
        if (!this.labelListEmpty) {
            const embeddedText = await this.get_embeddings(text, label, direction);
            this._mStatus = 2;        
            this.scratch_vm.emit(this.scratch_vm.constructor.PERIPHERAL_CONNECTED);
        }
    }


    /**
     * Embeds text and either adds examples to classifier or returns the predicted label
     * Changes text into a 2d tensor
     * @param text - the text inputted
     * @param label - the label to add the example to
     * @param direction - is either "example" when an example is being inputted or "predict" when a word to be classified is inputted
     * @returns if the direction is "predict" returns the predicted label for the text inputted
     */
    // TODO rename this function to better align with what it does
    async get_embeddings (text, label, direction) {
        // translates text from any language to english
        const newText = await this.getTranslate(text, 'en');

        if (!this.labelListEmpty) {
            // before going through with the expensive USE model loading
            //   check to see if we already have an embedding for this input
            let textEmbeddings = this.exampleEmbeddings[newText] || this.lastEmbedding[newText];
            if (!textEmbeddings) {
                let useModel = await this.useModelPromise;
                textEmbeddings = await this.embed(newText);
            }
            if (direction === "example") {
                await this.classifier.addExample(textEmbeddings, label);

                // save embeddings of examples for later use
                this.exampleEmbeddings[newText] = textEmbeddings;
                return "Inputting to classes";
    
            } else if (direction === "predict") {
                // TODO consider making this value the n-th root of the number of labels e.g. for 5 labels, take the 5th root
                const k = Math.sqrt(this.count);
                let prediction = await this.classifier.predictClass(textEmbeddings, k);
                console.log("get embeddings result:", prediction);
                this.predictedLabel = prediction.label;
                this.confidence = prediction.confidences[prediction.label];
                return [this.predictedLabel, this.confidence];
            }
        } else {
            return "No class inputted";
        }
    }
   /**
     * Exports the labels and examples in the form of a JSON file with the default name of "classifier-info.json"
     */
      exportClassifier () {
        // start of exporting csv

        // holds the formatted headers and data
        const headers = {'fields':[]}
        const data = [];
        let maximum = 0;
        // creates a dictionary of headers to put into the CSV file
        for (const label in this.scratch_vm.modelData.textData) {
            headers['fields'].push(String(label).replace(/,/g, ''));

            // finds the maximum row length
            if (this.scratch_vm.modelData.textData[label].length > maximum) {
                maximum = this.scratch_vm.modelData.textData[label].length;
            }
        }
        
        // formats data so each embedded array represents a row
        for (let i = 0; i < maximum; i++) {
            const temp = [];
            for (const label in this.scratch_vm.modelData.textData) {
                try {
                    temp.push(this.scratch_vm.modelData.textData[label][i].replace(/,/g, ''));
                } catch (error) {
                    temp.push(' ');
                }
            }
            // adds array to the total arrays
            data.push(temp);
        }

        // adds the formatted examples to the dictionary with the labels
        headers['data'] = data;

        // exports data to a CSV file
        const csv = Papa.unparse(headers); // converts to a CSV format
        const csvData = new Blob([csv], {type: 'text/csv;charset=utf-8;'});
        let csvURL = null;
        if (navigator.msSaveBlob) {
            csvURL = navigator.msSaveBlob(csvData,'classifier-export.csv');
        } else {
            csvURL = window.URL.createObjectURL(csvData);
        }
        let tempLink = document.createElement('a');

        tempLink.setAttribute('href', csvURL);
        tempLink.setAttribute('download', 'classifier-export.csv');
        tempLink.click();
        
    }

    /**
     * Loads the json document which contains labels and examples. Inputs the labels and examples into the classifier
     */
    async loadClassifier() { //loads classifier to project
        const self = this;
        const dataset = document.getElementById("imported-classifier").files[0];
        /**
         * parses through the json document and adds to the model textData and classifier
         * @param e the JSON document that we are loading
        */
        function receivedText(e) { 
            let lines = e.target.result;

            // check if JSON is in the correct form
            try {
                var newArr = JSON.parse(lines);
                self.clearAll();
                for (let label in newArr) {
                    if (newArr.hasOwnProperty(label)) {
                        let textExamples = newArr[label];
                        self.newLabel(label);
                        self.newExamples(textExamples, label);
                    }
                }
            } catch (err) {
                // check if spreadsheet CSV is in the correct form
                try {
                    // converts the csv file into JSON
                    let newArr = Papa.parse(lines,{ header: true, skipEmptyLines: 'greedy' });
                    self.clearAll();
                    // adds the labels to the classifier
                    for (let row in newArr.data) {
                        for (let col in newArr.data[row]) {
                            // if we have not already added the label
                            if (!(col in self.scratch_vm.modelData.textData)) { 
                                if (col) {
                                    self.newLabel(col);
                                }
                            }
                            // adds the examples to the classifier
                            if (newArr.data[row][col]) {
                                self.newExamples([newArr.data[row][col]],[col])
                            }
                        }
                    }
                } catch (err) {
                    console.log("CSV didn't work")
                }
                console.log("JSON and CSV didn't work");
            }
        
        }
        if (dataset !== undefined) {
            const fr = new FileReader();
            fr.onload = receivedText;
            fr.readAsText(dataset);
        }
    }
    getTranslate (words, language) {
        return Promise.resolve(words);
        // Don't remake the request if we already have the value.
        if (this._lastTextTranslated === words &&
            this._lastLangTranslated === language) {
            return this._translateResult;
        }

        const lang = language;

        let urlBase = `${serverURL}translate?language=`;
        urlBase += lang;
        urlBase += '&text=';
        urlBase += encodeURIComponent(words);

        const tempThis = this;
        const translatePromise = new Promise(resolve => {
            nets({
                url: urlBase,
                timeout: serverTimeoutMs
            }, (err, res, body) => {
                if (err) {
                    // log.warn(`error fetching translate result! ${res}`);
                    resolve('');
                    return '';
                }
                const translated = JSON.parse(body).result;
                tempThis._translateResult = translated;
                // Cache what we just translated so we don't keep making the
                // same call over and over.
                tempThis._lastTextTranslated = words;
                tempThis._lastLangTranslated = language;
                resolve(translated);
                return translated;
            });

        });
        translatePromise.then(translatedText => translatedText);
        return translatePromise;
    }
     
}

module.exports = Scratch3TextClassificationBlocks;
